{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from PIL import Image\n",
    "import base64\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b112bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process a page and return its text and page number\n",
    "def process_page(page):\n",
    "    page_text = page.get_text(\"text\")\n",
    "    return page_text, page.number\n",
    "\n",
    "\n",
    "def correct_text_segmentation(content_data, delimiter):\n",
    "    corrected_data = []\n",
    "    for page_text in content_data:\n",
    "        # Split the text using the delimiter\n",
    "        split_texts = page_text['text'].split(delimiter)\n",
    "\n",
    "        # The first part goes to the current page\n",
    "        corrected_data.append({\n",
    "            'text_on_page': page_text['text_on_page'],\n",
    "            'text': split_texts[0]\n",
    "        })\n",
    "\n",
    "        # If there's more content after the split, it goes to the next page\n",
    "        # Remove the 'delimiter +' from the following line to exclude the heading when saving to the JSON\n",
    "        if len(split_texts) > 1:\n",
    "            corrected_data.append({\n",
    "                'text_on_page': page_text['text_on_page'] + 1,\n",
    "                'text': split_texts[1]\n",
    "            })\n",
    "    return corrected_data\n",
    "\n",
    "\n",
    "def merge_duplicate_pages(content_data):\n",
    "    # Create a dictionary to store consolidated texts\n",
    "    consolidated_data = {}\n",
    "\n",
    "    for entry in content_data:\n",
    "        page_num = entry[\"text_on_page\"]\n",
    "        text = entry[\"text\"].strip()  # Remove any leading/trailing white spaces\n",
    "        \n",
    "        # Skip the entry if the text is empty\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if page_num not in consolidated_data:\n",
    "            consolidated_data[page_num] = text\n",
    "        else:\n",
    "            # Add a space before appending the next text\n",
    "            consolidated_data[page_num] += \" \" + text\n",
    "\n",
    "    # Convert the consolidated data dictionary back to a list format\n",
    "    merged_data = [{\"text_on_page\": k, \"text\": v} for k, v in consolidated_data.items()]\n",
    "    \n",
    "    # Optionally, sort the list by 'text_on_page' if needed\n",
    "    merged_data.sort(key=lambda x: x[\"text_on_page\"])\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "# Define a regular expression pattern to match specific text sections\n",
    "\n",
    "# pattern = re.compile(r\"\\(E-01:\\)[\\s\\S]*?(?=\\(E-01:\\)|$)\")  #ARUBA\n",
    "pattern = re.compile(r\"\\(210\\)[\\s\\S]*?(?=\\(210\\)|$)\")  #BHUTAN\n",
    "# pattern = re.compile(r\"\\(111\\)[\\s\\S]*?(?=\\(111\\)|$)\")  #MONGOLIA\n",
    "\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_file_path = \"BT20211006-98.pdf\"\n",
    "pdf_document = fitz.open(pdf_file_path)\n",
    "\n",
    "# This dictionary will hold all extracted text with corresponding page numbers\n",
    "text_pages = {}\n",
    "for page in pdf_document:\n",
    "    text, pn = process_page(page)\n",
    "    text_pages[pn] = text\n",
    "\n",
    "# Concatenate the text from all pages\n",
    "extracted_text = \" \".join(text_pages.values())\n",
    "\n",
    "# Find matches in the concatenated text\n",
    "matches = pattern.findall(extracted_text)\n",
    "\n",
    "# Create a dictionary to store section data and initialize section number\n",
    "section_data = {}\n",
    "section_num = 1\n",
    "\n",
    "# Initialize a list to store section content data\n",
    "section_content_data = []\n",
    "\n",
    "# Iterate through the matches\n",
    "for match in matches:\n",
    "    start_index = extracted_text.find(match)\n",
    "\n",
    "    # Determine the pages the match spans\n",
    "    # Create an empty list to store the page numbers where the match appears\n",
    "    page_nums = []\n",
    "    # Initialize a variable to keep track of the current position in the extracted text\n",
    "    current_pos = 0 \n",
    "    # Iterate through each page and its corresponding text\n",
    "    for pn, txt in text_pages.items():\n",
    "        # Update the current position by adding the length of the text on the current page\n",
    "        current_pos += len(txt)\n",
    "        if start_index < current_pos:\n",
    "            # If the starting index of the match is less than the current position, it means the match spans this page\n",
    "            # Add the page number to the list of page numbers where the match appears\n",
    "            # Increase the pn (page_numer) by 1 to match page number in PDF\n",
    "#             new_pn_matching_PDF = pn + 1\n",
    "            page_nums.append(pn)\n",
    "            # If the current position is greater than the end of the match, \n",
    "            # it means the match doesn't span further pages\n",
    "            if current_pos > start_index + len(match):\n",
    "                break\n",
    "\n",
    "    # Skip a specific section \n",
    "    if \"(511) \\u2013 NICE classification for goods and services\" in match:\n",
    "        continue\n",
    "\n",
    "    if section_num not in section_data:\n",
    "        section_data[section_num] = {\"page_number\": [pn + 1 for pn in page_nums] # Adding 1 to adjust the page number\n",
    "                                     , \"content\": [],\n",
    "                                     \"content_data\": []\n",
    "                                    }\n",
    "\n",
    "    # Split the matched text into lines and add them to the section data\n",
    "    content_lines = [line for line in match.split(\"\\n\") if line.strip()]\n",
    "    section_data[section_num][\"content\"].extend(content_lines)\n",
    "\n",
    "    # Initialize a pointer for the current start of the segment\n",
    "    segment_start = start_index\n",
    "\n",
    "    # Temporary storage for the segmented content for this match\n",
    "    current_content_data = []\n",
    "\n",
    "    # Iterate through the pages the match spans\n",
    "    for pn in page_nums:\n",
    "        # Determine where the segment for this page ends\n",
    "        segment_end = min(segment_start + len(text_pages[pn]), start_index + len(match))\n",
    "\n",
    "        # Check if segment_end is cutting a word. If yes, move it back to the last space.\n",
    "        while extracted_text[segment_end] not in [\" \", \"\\n\", \"\\t\"] and segment_end > segment_start:\n",
    "            segment_end -= 1\n",
    "\n",
    "        # Extract the segment of the match that lies within this page\n",
    "        segment = extracted_text[segment_start:segment_end]\n",
    "\n",
    "        # If this segment contains part of the match, add it to the current_content_data\n",
    "        if segment.strip():\n",
    "            current_content_data.append({\n",
    "                \"text_on_page\": pn + 1, # Adding 1 to adjust the page number\n",
    "                \"text\": segment.strip()\n",
    "            })\n",
    "\n",
    "        # Update segment_start for the next page\n",
    "        segment_start = segment_end\n",
    "    \n",
    "    # Add the segmented content data to the section data\n",
    "    section_data[section_num][\"content_data\"].extend(current_content_data)\n",
    "    \n",
    "    # Call the correction function for the current section\n",
    "    #  Removing the HEADING from the extracted text\n",
    "    delimiter = 'Successfull Examination Marks'\n",
    "    section_data[section_num][\"content_data\"] = correct_text_segmentation(section_data[section_num][\"content_data\"], delimiter)\n",
    "    \n",
    "    # Call the merge_duplicate_pages function to join text on the same page into one\n",
    "    section_data[section_num][\"content_data\"] = merge_duplicate_pages(section_data[section_num][\"content_data\"])\n",
    "    \n",
    "    # Split the text into lines (segments) for each entry in content_data\n",
    "    for entry in section_data[section_num][\"content_data\"]:\n",
    "        text = entry[\"text\"]\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Remove empty strings from the 'lines' list\n",
    "        lines = [line for line in lines if line.strip() != \"\"]\n",
    "        \n",
    "        # Update the \"text\" value for the current entry to contain the split lines\n",
    "        entry[\"text\"] = lines\n",
    "        \n",
    "    section_num += 1\n",
    "\n",
    "# Serialize the section data to a JSON format\n",
    "output_data = json.dumps(section_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Write the JSON data to a file\n",
    "with open(f\"{pdf_file_path}_result_latest_version.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json_file.write(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48542469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_coordinates_from_pdf(pdf_path, extracted_text_json_path, output_json_path):\n",
    "    extracted_text_dict = {}\n",
    "    \n",
    "    # Load the JSON file containing the previously extracted text (if it exists)\n",
    "    try:\n",
    "        with open(extracted_text_json_path, 'r', encoding='utf-8') as json_file:\n",
    "            existing_data = json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file is not found\n",
    "        print(\"[-] Existing file not found\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "         # Handle other exceptions (e.g., file read errors) and print the error message\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # Extract text from the PDF and store it in extracted_text_dict\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Initialize variables to store the minimum and maximum coordinates\n",
    "        min_x0, min_y0, max_x1, max_y1 = float('inf'), float('inf'), -float('inf'), -float('inf')\n",
    "\n",
    "        # Iterate through each page in the PDF\n",
    "        for page_number, page in enumerate(pdf.pages, start=1):\n",
    "            # Initialize a dictionary to store extracted text for the current page\n",
    "            extracted_text_dict[str(page_number)] = {\n",
    "                \"page_number\": page_number,\n",
    "                \"extracted_texts\": []\n",
    "            }\n",
    "            print(\"[*] page number:\", page_number)\n",
    "            print(\"----------------------\")\n",
    "\n",
    "            # Retrieve the current page (0-based index)\n",
    "            page = pdf.pages[page_number - 1]  # Pages are 0-based index\n",
    "\n",
    "            # Extract words from the page with their bounding box coordinates\n",
    "            words_with_bounding_box = page.extract_words()\n",
    "\n",
    "            # Initialize a dictionary to group words into lines based on y-coordinates\n",
    "            lines = {}\n",
    "            for word in words_with_bounding_box:\n",
    "                # Round y-coordinate to handle floating-point inaccuracies\n",
    "                y0 = round(word[\"top\"], 2)\n",
    "                y1 = round(word[\"bottom\"], 2)\n",
    "                if (y0, y1) in lines:\n",
    "                    # Append word data to an existing line\n",
    "                    lines[(y0, y1)][\"text\"].append(word[\"text\"])\n",
    "                    lines[(y0, y1)][\"x0\"].append(word[\"x0\"])\n",
    "                    lines[(y0, y1)][\"x1\"].append(word[\"x1\"])\n",
    "                else:\n",
    "                    # Create a new line entry\n",
    "                    lines[(y0, y1)] = {\n",
    "                        \"text\": [word[\"text\"]],\n",
    "                        \"x0\": [word[\"x0\"]],\n",
    "                        \"x1\": [word[\"x1\"]],\n",
    "                    }\n",
    "\n",
    "            # Sort lines by y-coordinates\n",
    "            sorted_lines = sorted(lines.items(), key=lambda x: x[0])\n",
    "\n",
    "            # Iterate through existing data entries\n",
    "            for page_data_key, page_data in existing_data.items():\n",
    "                # Skip this entry if it has no 'content_data'\n",
    "                if not page_data.get(\"content_data\"):\n",
    "                    continue\n",
    "\n",
    "                # Iterate through content entries for the current page\n",
    "                for content_entry in page_data[\"content_data\"]:\n",
    "                    # Get the start and end patterns\n",
    "                    start = content_entry[\"text\"][0]\n",
    "                    end = content_entry[\"text\"][-1]\n",
    "\n",
    "                    # Initialize a flag to track if a match is found\n",
    "                    match = False\n",
    "\n",
    "                    # Initialize variables to calculate the minimum and maximum coordinates\n",
    "                    min_x0, min_y0, max_x1, max_y1 = float('inf'), float('inf'), -float('inf'), -float('inf')\n",
    "\n",
    "                    # Iterate through sorted lines\n",
    "                    for (y0, y1), line_info in sorted_lines:\n",
    "                        text = \" \".join(line_info[\"text\"])\n",
    "\n",
    "                        if match:\n",
    "                            # to get the maximum coordinates of the text\n",
    "                            # determine the smallest x0 and y0 coordinates (the top-left corner) and \n",
    "                            # largest x1 and y1 coordinates (the bottom-right corner) for a text block\n",
    "                            # These are then used to calculate the overall coordinates for the entire text extracted between the \"start\" and \"end\" patterns.\n",
    "                            min_x0 = min(min_x0, min(line_info[\"x0\"]))\n",
    "                            min_y0 = min(min_y0, y0)\n",
    "                            max_x1 = max(max_x1, max(line_info[\"x1\"]))\n",
    "                            max_y1 = max(max_y1, y1)\n",
    "\n",
    "                         # Compare text to the start pattern\n",
    "                        start_found = [i.replace(\"\\n\", '').lower() for i in text.split(\" \") if i] == [\n",
    "                            i.replace(\"\\n\", '').lower() for i in start.split(\" \") if i]\n",
    "\n",
    "                        # Mark a match when the start pattern is found\n",
    "                        if start_found and not match:\n",
    "                            match = True\n",
    "\n",
    "                        # Compare text to the end pattern\n",
    "                        end_found = [i.replace(\"\\n\", '').lower() for i in text.split(\" \") if i] == [\n",
    "                            i.replace(\"\\n\", '').lower() for i in end.split(\" \") if i]\n",
    "\n",
    "                        if end_found and match:\n",
    "                            # Add the extracted text and its coordinates to the dictionary\n",
    "                            extracted_text_dict[str(page_number)][\"extracted_texts\"].append({\n",
    "                                \"extracted_text\": content_entry['text'],\n",
    "                                \"x0\": min_x0,\n",
    "                                \"y0\": min_y0,\n",
    "                                \"x1\": max_x1,\n",
    "                                \"y1\": max_y1,\n",
    "                            })\n",
    "                            # Print a message indicating that the text has been successfully extracted\n",
    "                            match = False\n",
    "                            print(\"[+] extracted the text => \",\n",
    "                                  '\\n'.join(content_entry['text']), \"\\n\")\n",
    "\n",
    "                            # Update the content_data entry with coordinates\n",
    "                            content_entry[\"coordinates\"] = {\n",
    "                                \"x0\": min_x0,\n",
    "                                \"y0\": min_y0,\n",
    "                                \"x1\": max_x1,\n",
    "                                \"y1\": max_y1\n",
    "                            }\n",
    "                            break\n",
    "\n",
    "    # Save the modified JSON file with coordinates added\n",
    "    with open(extracted_text_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(existing_data, json_file, indent=4, separators=(',', ': '))\n",
    "\n",
    "    # Save the defined coordinates to a new JSON file with formatting\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as output_json_file:\n",
    "        json.dump(extracted_text_dict, output_json_file, indent=4, separators=(',', ': '))\n",
    "\n",
    "        \n",
    "# Example usage:\n",
    "pdf_path = pdf_file_path  # Replace with the path to your PDF file, THIS VARIABLE IS IN THE STARTING OF THE SCRIPT\n",
    "extracted_text_json_path = f\"{pdf_file_path}_result_latest_version.json\"  # This is the JSON input file\n",
    "output_json_path = f\"{pdf_file_path}_defined_coordinates.json\"  # This file is created as the output\n",
    "define_coordinates_from_pdf(\n",
    "    pdf_path, extracted_text_json_path, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f835ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_rectangles = {}\n",
    "\n",
    "# The coordinates are in the format (x0, y0, x1, y1) - top left and bottom right corners.\n",
    "# x0 is the horizontal coordinate (from the left) of the top-left corner. [LEFT VERTICLE LINE goes LEFT(lower number) or RIGHT (higher number)]\n",
    "# y0 is the vertical coordinate (from the top) of the top-left corner. [TOP HORIZONTAL LINE goes UP (lower number) and DOWN(higher number) ]\n",
    "# x1 is the horizontal coordinate (from the left) of the bottom-right corner. [RIGHT VERTICLE LINE goes LEFT (lower number) or RIGHT (higher number)]\n",
    "# y1 is the vertical coordinate (from the top) of the bottom-right corner. [BOTTOM HORIZONTAL LINE goes UP(lower number) or DOWN(higher number)]\n",
    "\n",
    "def extract_and_save_image(pdf_file, coordinates_file, output_folder, resolution=200):\n",
    "    padding = 14.5\n",
    "    try:\n",
    "        # Ensure the output folder exists\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        \n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(pdf_file)\n",
    "        \n",
    "        # Load coordinates from the JSON file\n",
    "        with open(coordinates_file, \"r\") as json_file:\n",
    "            coordinates_data = json.load(json_file)\n",
    "        \n",
    "        # Initialize a list to store extracted image data for each page\n",
    "        extracted_images_per_page = {}\n",
    "        for key, entry in coordinates_data.items():\n",
    "            page_number = entry[\"page_number\"]\n",
    "\n",
    "            page = pdf_document[page_number - 1]\n",
    "            page_width = page.rect.width  # Get the page width\n",
    "            \n",
    "            for i, text in enumerate(entry[\"extracted_texts\"]):\n",
    "                x0 = text[\"x0\"] - padding\n",
    "                y0 = text[\"y0\"] - padding\n",
    "                x1 = page_width - 40  # USING PAGE WIDTH TO AVOID ERROR OF MISSING DATA \n",
    "                y1 = text[\"y1\"] + padding\n",
    "                page = pdf_document[page_number - 1] \n",
    "                rect = fitz.Rect(x0, y0, x1, y1)\n",
    "#                 print(rect)\n",
    "                image = page.get_pixmap(\n",
    "                    matrix=fitz.Matrix(resolution / 72.0, resolution / 72.0), clip=rect\n",
    "                )\n",
    "                # Example check before processing each rectangle\n",
    "                if x0 < 0 or y0 < 0 or x1 <= x0 or y1 <= y0:\n",
    "                    print(f\"Skipping invalid rectangle coordinates: {x0}, {y0}, {x1}, {y1}\")\n",
    "                    \n",
    "                    # Increment the skipped rectangle count for this page\n",
    "                    if page_number not in skipped_rectangles:\n",
    "                        skipped_rectangles[page_number] = 0\n",
    "                    skipped_rectangles[page_number] += 1\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                # SAVE THE EXTRACTED IMAGE\n",
    "                image_filename = os.path.join(output_folder, f\"{pdf_file}_page_{page_number}_section_{i}.png\")\n",
    "                image.save(image_filename)\n",
    "                print(f\"Extracted image saved as {image_filename}\")\n",
    "\n",
    "                # Now, convert the PNG image to GIF format using Pillow\n",
    "                gif_filename = os.path.join(output_folder, f\"{pdf_file}_page_{page_number}_section_{i}.gif\")\n",
    "                image_pil = Image.open(image_filename)\n",
    "                image_pil.save(gif_filename, \"GIF\")\n",
    "        \n",
    "                print(f\"Converted PNG to GIF: {gif_filename}\")\n",
    "        \n",
    "                # Encode the image as a base64 string\n",
    "                with open(image_filename, \"rb\") as image_file:\n",
    "                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "                # Get just the filename without the path\n",
    "                base_filename = os.path.basename(image_filename)\n",
    "        \n",
    "                # Create a dictionary with the base filename as the key and base64 data as the value\n",
    "                image_data = {\n",
    "                    base_filename: base64_image\n",
    "                }\n",
    "                \n",
    "                # Save the base64 data in a JSON file\n",
    "                base64_filename = os.path.join(output_folder, f\"{pdf_file}_page_{page_number}_section_{i}.json\")\n",
    "                with open(base64_filename, \"w\") as json_file:\n",
    "                    json.dump(image_data, json_file, indent=4)\n",
    "\n",
    "                # Append the base64 data to the original coordinates_data\n",
    "                text[\"base64\"] = base64_image\n",
    "            \n",
    "        # Save the updated coordinates_data back to the coordinates file\n",
    "        with open(coordinates_file, \"w\") as json_file:\n",
    "            json.dump(coordinates_data, json_file, indent=4)\n",
    "        \n",
    "        # Now, outside of the for-loop, after all processing is done, we report the skipped rectangles\n",
    "        for page, count in skipped_rectangles.items():\n",
    "            print(f\"SKIPPED {count} RECTANGLES ON PAGE {page} of {pdf_file_path}\")\n",
    "        \n",
    "        # WRITE THE MISSED RECTANGLES IN A .TXT FILE\n",
    "        with open(f'skipped_rectangles_in_{pdf_file_path}.txt', 'w') as f:\n",
    "            for page, count in skipped_rectangles.items():\n",
    "                f.write(f\"Page {page}: Skipped {count} \\n\")\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_file = pdf_file_path #THIS VARIABLE IS IN THE STARTING OF THE SCRIPT\n",
    "    coordinates_file = output_json_path # DEFINED COORDINATES\n",
    "    output_folder = f\"OUTPUT_SCREENSHOTS_OF_APPLICATIONS_FOR_{pdf_file_path}\"\n",
    "    resolution = 200\n",
    "    extract_and_save_image(pdf_file, coordinates_file, output_folder, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70962098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
